
INFO:root:Collecting batch trajectories...
INFO:root:Updating network parameter...
Traceback (most recent call last):
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 471, in <module>
    agent.learn()
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 289, in learn
    policy_loss, value_loss = self.train(obs, rewards, advantages, log_probs, curr_log_probs, self.epsilon)
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 261, in train
    policy_loss = self.policy_net.loss(advantages, batch_log_probs, curr_log_probs, epsilon)
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/PPO/ppo_torch/ppo_continuous.py", line 85, in loss
    clip_1 = ratio * advantages
RuntimeError: The size of tensor a (2000) must match the size of tensor b (200) at non-singleton dimension 0