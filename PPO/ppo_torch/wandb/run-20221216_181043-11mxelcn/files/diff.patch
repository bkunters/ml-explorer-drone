diff --git a/PPO/ppo_torch/ppo_continuous.py b/PPO/ppo_torch/ppo_continuous.py
index ca4ac79..009c2b1 100644
--- a/PPO/ppo_torch/ppo_continuous.py
+++ b/PPO/ppo_torch/ppo_continuous.py
@@ -160,9 +160,6 @@ class PPO_PolicyGradient:
         log_prob = dist.log_prob(actions)
         return values, log_prob
 
-    def get_value(self, obs):
-        return self.value_net(obs).squeeze()
-
     def step(self, obs):
         """ Given an observation, get action and probabilities from policy network (actor)"""
         action_dist = self.get_continuous_policy(obs) 
@@ -212,9 +209,9 @@ class PPO_PolicyGradient:
             done = False
 
             # Run episode for a fixed amount of timesteps
-            # to keep rollout size fixed
+            # to keep rollout size fixed and episodes independent
             for ep_t in range(self.max_trajectory_size):
-                # render gym env
+                # render gym envs
                 if render:
                     self.env.render(mode='human')
                 
@@ -237,21 +234,8 @@ class PPO_PolicyGradient:
 
                 # break out of loop if episode is terminated
                 if done:
-                    # # STEP 4: Calculate cummulated reward
-                    # total_reward = sum(trajectory_rewards) # TODO: Is this correct? 
-                    # # STEP 5: compute advantage estimates A_t at timestep num_steps
-                    # # calculate advantage in different ways --> GAE can be done later
-                    # # keep the rollout size fixed --> episodes should stay independent
-                    # # log when episode end
-                    # advantage = self.advantage_estimate(np.array(total_reward, dtype=np.float32), \
-                    #                                     np.array(trajectory_values, dtype=np.float32))
-                    # trajectory_advantages = advantage
-
-                    # self.num_episodes += 1
-                    
                     # # reset values
-                    # trajectory_values = []
-                    # obs = self.env.reset()
+                    obs = self.env.reset()
                     ep_rewards = []
                     break
             
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 5b5538e..a0306f6 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20221215_130627-g5gz7u2b/logs/debug-internal.log
\ No newline at end of file
+run-20221215_231144-3fnhylxq/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 3348e0e..21432b6 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20221215_130627-g5gz7u2b/logs/debug.log
\ No newline at end of file
+run-20221215_231144-3fnhylxq/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 59dc0e4..bba22d0 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20221215_130627-g5gz7u2b
\ No newline at end of file
+run-20221215_231144-3fnhylxq
\ No newline at end of file
