
Collecting batch trajectories...
Mean cumulative reward: 13.909090909090908
Updating the neural networks...
Epoch: 4, Policy loss: -0.007602682337164879
Epoch: 4, Value loss: 0.9642396569252014
Total time steps: 153
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Flatten object at 0x104cceaf0>, because it is not built.
WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Flatten object at 0x12bc2c1f0>, because it is not built.
WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Flatten object at 0x12bc2c1f0>, because it is not built.
WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn, dense_5_layer_call_and_return_conditional_losses, dense_6_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.
Collecting batch trajectories...
Mean cumulative reward: 12.0
Updating the neural networks...
Epoch: 4, Policy loss: -0.0033088193740695715
Epoch: 4, Value loss: 0.9762973189353943
Total time steps: 405
Collecting batch trajectories...
Mean cumulative reward: 5.548387096774194
Updating the neural networks...
Epoch: 4, Policy loss: -0.005760829895734787
Epoch: 4, Value loss: 0.9525473117828369
Total time steps: 577
Collecting batch trajectories...
Mean cumulative reward: 4.682926829268292
Updating the neural networks...
Epoch: 4, Policy loss: -0.013515334576368332
Epoch: 4, Value loss: 0.8473803400993347
Total time steps: 769
Collecting batch trajectories...
Mean cumulative reward: 4.431372549019608
Updating the neural networks...
Epoch: 4, Policy loss: -0.009711703285574913
Epoch: 4, Value loss: 1.0229101181030273
Total time steps: 995
Collecting batch trajectories...
Mean cumulative reward: 5.524590163934426
Updating the neural networks...
Epoch: 4, Policy loss: -0.004328524693846703
Epoch: 4, Value loss: 1.0148663520812988
Total time steps: 1332
Collecting batch trajectories...
Mean cumulative reward: 5.774647887323944
Updating the neural networks...
Epoch: 4, Policy loss: -0.007662054616957903
Epoch: 4, Value loss: 0.9665590524673462
Total time steps: 1742
Collecting batch trajectories...
Mean cumulative reward: 8.444444444444445
Updating the neural networks...
Epoch: 4, Policy loss: -0.003598929150030017
Epoch: 4, Value loss: 1.1391606330871582
Total time steps: 2426
Collecting batch trajectories...
Mean cumulative reward: 6.43956043956044
Updating the neural networks...
Epoch: 4, Policy loss: -0.0057134805247187614
Epoch: 4, Value loss: 0.9894307851791382
Total time steps: 3012
Collecting batch trajectories...
Mean cumulative reward: 6.3861386138613865
Updating the neural networks...
Epoch: 4, Policy loss: -0.001992566743865609
Epoch: 4, Value loss: 1.0290321111679077
Total time steps: 3657
Collecting batch trajectories...
Mean cumulative reward: 5.54954954954955
Updating the neural networks...
Epoch: 4, Policy loss: -0.002792595885694027
Epoch: 4, Value loss: 0.9953902363777161
Total time steps: 4273
Collecting batch trajectories...
Mean cumulative reward: 4.231404958677686
Updating the neural networks...
Epoch: 4, Policy loss: -0.004627938382327557
Epoch: 4, Value loss: 0.9950392842292786
Total time steps: 4785
Collecting batch trajectories...
Mean cumulative reward: 6.213740458015267
Updating the neural networks...
Epoch: 4, Policy loss: -0.006015686318278313
Epoch: 4, Value loss: 0.949237048625946
Total time steps: 5599
Collecting batch trajectories...
Mean cumulative reward: 6.9787234042553195
Updating the neural networks...
Epoch: 4, Policy loss: 4.821591574000195e-05
Epoch: 4, Value loss: 1.0137290954589844
Total time steps: 6583
Collecting batch trajectories...
Mean cumulative reward: 6.377483443708609
Updating the neural networks...
Epoch: 4, Policy loss: -0.0022631119936704636
Epoch: 4, Value loss: 1.0062757730484009
Total time steps: 7546
Collecting batch trajectories...
Mean cumulative reward: 4.900621118012422
Updating the neural networks...
Epoch: 4, Policy loss: -0.0009081067400984466
Epoch: 4, Value loss: 0.9664620161056519
Total time steps: 8335
Collecting batch trajectories...
Mean cumulative reward: 4.637426900584796
Updating the neural networks...
Epoch: 4, Policy loss: -0.005882652010768652
Epoch: 4, Value loss: 0.9567967057228088
Total time steps: 9128
Collecting batch trajectories...
Mean cumulative reward: 4.895027624309392
Updating the neural networks...
Epoch: 4, Policy loss: -0.00695725716650486
Epoch: 4, Value loss: 0.9610755443572998
Total time steps: 10014
Collecting batch trajectories...
Mean cumulative reward: 5.204188481675392
Updating the neural networks...
Epoch: 4, Policy loss: -0.0023140234407037497
Epoch: 4, Value loss: 0.9315478801727295
Total time steps: 11008
Collecting batch trajectories...
Mean cumulative reward: 5.08955223880597
Updating the neural networks...
Epoch: 4, Policy loss: 9.531755495117977e-05
Epoch: 4, Value loss: 1.003279685974121
Total time steps: 12031
Collecting batch trajectories...
Mean cumulative reward: 4.867298578199052
Updating the neural networks...
Epoch: 4, Policy loss: -0.0016240355325862765
Epoch: 4, Value loss: 1.0616626739501953
Total time steps: 13058
Collecting batch trajectories...
Mean cumulative reward: 5.239819004524887
Updating the neural networks...
Epoch: 4, Policy loss: 0.0005992006626911461
Epoch: 4, Value loss: 0.9871753454208374
Total time steps: 14216
Collecting batch trajectories...
Mean cumulative reward: 4.489177489177489
Updating the neural networks...
Epoch: 4, Policy loss: -0.0008262001210823655
Epoch: 4, Value loss: 0.9724526405334473
Total time steps: 15253
Collecting batch trajectories...
Mean cumulative reward: 5.5103734439834025
Updating the neural networks...
Epoch: 4, Policy loss: 4.2583749745972455e-05
Epoch: 4, Value loss: 0.9642656445503235
Total time steps: 16581
Collecting batch trajectories...
Mean cumulative reward: 4.362549800796812
Updating the neural networks...
Epoch: 4, Policy loss: -0.0015046336920931935
Epoch: 4, Value loss: 0.9288811087608337
Total time steps: 17676
Collecting batch trajectories...
Mean cumulative reward: 3.9578544061302683
Updating the neural networks...
Epoch: 4, Policy loss: -0.005624696146696806
Epoch: 4, Value loss: 1.0335081815719604
Total time steps: 18709
Collecting batch trajectories...
Mean cumulative reward: 3.6937269372693726
Updating the neural networks...
Epoch: 4, Policy loss: -0.0009895337279886007
Epoch: 4, Value loss: 0.9965487718582153
Total time steps: 19710
Collecting batch trajectories...
Mean cumulative reward: 3.9537366548042705
Updating the neural networks...
Epoch: 4, Policy loss: -0.002937065903097391
Epoch: 4, Value loss: 0.9934956431388855
Total time steps: 20821
Collecting batch trajectories...
Mean cumulative reward: 3.649484536082474
Updating the neural networks...
Epoch: 4, Policy loss: -0.005038565956056118
Epoch: 4, Value loss: 0.9683434367179871
Total time steps: 21883
Collecting batch trajectories...
Mean cumulative reward: 4.3754152823920265
Updating the neural networks...
Epoch: 4, Policy loss: 0.0009770732140168548
Epoch: 4, Value loss: 1.0688894987106323
Total time steps: 23200
Collecting batch trajectories...
Mean cumulative reward: 3.672025723472669
Updating the neural networks...
Epoch: 4, Policy loss: -0.003722846508026123
Epoch: 4, Value loss: 1.0115386247634888
Total time steps: 24342
Collecting batch trajectories...
Mean cumulative reward: 4.074766355140187
Updating the neural networks...
Epoch: 4, Policy loss: -0.002130180597305298
Epoch: 4, Value loss: 1.0014129877090454
Total time steps: 25650
Collecting batch trajectories...
Mean cumulative reward: 3.806646525679758
Updating the neural networks...
Epoch: 4, Policy loss: -0.001261046389117837
Epoch: 4, Value loss: 0.9883337616920471
Total time steps: 26910
Collecting batch trajectories...
Mean cumulative reward: 3.595307917888563
Updating the neural networks...
Epoch: 4, Policy loss: 0.0005717593594454229
Epoch: 4, Value loss: 0.9797219038009644
Total time steps: 28136
Collecting batch trajectories...
Mean cumulative reward: 3.5384615384615383
Updating the neural networks...
Epoch: 4, Policy loss: -0.00032526589347980917
Epoch: 4, Value loss: 0.9535075426101685
Total time steps: 29378
Collecting batch trajectories...
Mean cumulative reward: 3.6177285318559558
Updating the neural networks...
Epoch: 4, Policy loss: -0.0019752788357436657
Epoch: 4, Value loss: 0.9594388008117676
Total time steps: 30684
Collecting batch trajectories...
Mean cumulative reward: 3.490566037735849
Updating the neural networks...
Epoch: 4, Policy loss: -0.0023116576485335827
Epoch: 4, Value loss: 1.0197561979293823
Total time steps: 31979
Collecting batch trajectories...
Mean cumulative reward: 3.545931758530184
Updating the neural networks...
Epoch: 4, Policy loss: -0.0016258963150903583
Epoch: 4, Value loss: 0.9754695892333984
Total time steps: 33330
Collecting batch trajectories...
Mean cumulative reward: 3.0971867007672635
Updating the neural networks...
Epoch: 4, Policy loss: -0.0010438772151246667
Epoch: 4, Value loss: 1.017093300819397
Total time steps: 34541
Collecting batch trajectories...
Mean cumulative reward: 3.546134663341646
Updating the neural networks...
Epoch: 4, Policy loss: -0.0020055361092090607
Epoch: 4, Value loss: 0.9280919432640076
Total time steps: 35963
Collecting batch trajectories...
Mean cumulative reward: 3.6034063260340634
Updating the neural networks...
Epoch: 4, Policy loss: -0.0010602783877402544
Epoch: 4, Value loss: 0.9521893262863159
Total time steps: 37444
Collecting batch trajectories...
Mean cumulative reward: 3.817102137767221
Updating the neural networks...
Epoch: 4, Policy loss: -0.000934542971663177
Epoch: 4, Value loss: 1.0230870246887207
Total time steps: 39051
Collecting batch trajectories...
Traceback (most recent call last):
  File "/Volumes/Work Disk Janina Mattes/DEV/University/ASP/ml-explorer-drone/ppo_impl/ppo_tf/ppo_tf.py", line 192, in <module>
    observation, reward, terminated, truncated, info = env.step(current_action)
  File "/Users/janinaalicamattes/miniforge3/envs/tensorflow_m1_ppo/lib/python3.9/site-packages/gym/wrappers/time_limit.py", line 50, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/Users/janinaalicamattes/miniforge3/envs/tensorflow_m1_ppo/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py", line 37, in step
    return self.env.step(action)
  File "/Users/janinaalicamattes/miniforge3/envs/tensorflow_m1_ppo/lib/python3.9/site-packages/gym/wrappers/env_checker.py", line 39, in step
    return self.env.step(action)
  File "/Users/janinaalicamattes/miniforge3/envs/tensorflow_m1_ppo/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py", line 187, in step
    self.render()
  File "/Users/janinaalicamattes/miniforge3/envs/tensorflow_m1_ppo/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py", line 298, in render
    self.clock.tick(self.metadata["render_fps"])
KeyboardInterrupt